{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NaN NaN NaN NaN NaN\n",
      "30 19 19.7336 37.0933 71.76 2.69964\n",
      "30 25 19.616 37.5737 97.52 2.69964\n",
      "30 24 1\n"
     ]
    }
   ],
   "source": [
    "data_file = \"labapp3-data-new.txt\"\n",
    "f = open(data_file, \"r\").read()\n",
    "print(f[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data attributes\n",
    "#time nodeid temperature humidity light voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2313683/2313683 [00:14<00:00, 162854.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[30.0, 19.0, 19.7336, 37.0933, 71.76, 2.69964],\n",
       " [30.0, 25.0, 19.616, 37.5737, 97.52, 2.69964],\n",
       " [30.0, 24.0, 19.7728, 37.162, 143.52, 2.71196],\n",
       " [30.0, 3.0, 20.204, 36.8871, 50.6, 2.69964],\n",
       " [30.0, 6.0, 20.4196, 36.6118, 121.44, 2.65143],\n",
       " [30.0, 26.0, 19.4494, 39.62, 121.44, 2.69964],\n",
       " [30.0, 27.0, 20.1354, 37.162, 79.12, 2.71196],\n",
       " [30.0, 31.0, 19.5572, 39.0763, 150.88, 2.69964],\n",
       " [30.0, 34.0, 19.4298, 39.0763, 60.72, 2.68742],\n",
       " [30.0, 43.0, 19.7532, 35.853, 478.4, 2.65143]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = f.split(\"\\n\")\n",
    "X = []\n",
    "\n",
    "for i in tqdm(data):\n",
    "    try:\n",
    "        X.append([float(x) for x in i.split(\" \")])\n",
    "        # check if list has nan values\n",
    "        if np.isnan(X[-1]).any():\n",
    "            X.pop()\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        print(i)\n",
    "X[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.03791455 0.02610045 0.         0.01777681]\n"
     ]
    }
   ],
   "source": [
    "temp = X[:10]\n",
    "#get the last 4 values of this list normalized\n",
    "MIN_S = np.min(np.array(temp), axis=0)\n",
    "MAX_S = np.max(np.array(temp), axis=0)\n",
    "\n",
    "print((np.array(temp[3])-MIN_S)/MAX_S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2219803/2219803 [00:08<00:00, 273310.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "data_dict = defaultdict(dict)\n",
    "for dt in tqdm(X):\n",
    "    # check if dt has any nan value\n",
    "    #if np.isnan(dt).any():\n",
    "    if np.isnan(dt[1]):\n",
    "        continue\n",
    "    if int(dt[1]) in data_dict[int(dt[0])].keys():\n",
    "        data_dict[int(dt[0])][int(dt[1])].append(dt[2:])\n",
    "    else:\n",
    "        data_dict[int(dt[0])][int(dt[1])] = dt[2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: [19.3318, 38.9401, 48.76, 2.68742],\n",
       " 29: [18.8222, 40.3652, 180.32, 2.67532],\n",
       " 27: [19.1848, 39.3143, 79.12, 2.69964],\n",
       " 42: [17.8912, 40.399, 478.4, 2.67532],\n",
       " 18: [18.8712, 38.8379, 57.04, 2.50599],\n",
       " 17: [18.6556, 39.5521, 32.2, 2.57108],\n",
       " 4: [19.7336, 39.4842, 97.52, 2.65143],\n",
       " 11: [18.5184, 41.7133, 861.12, 2.67532],\n",
       " 45: [18.2342, 38.3263, 172.96, 2.67532],\n",
       " 19: [18.9104, 38.872, 71.76, 2.68742],\n",
       " 44: [17.6364, 40.6693, 97.52, 2.67532],\n",
       " 40: [18.6458, 39.62, 172.96, 2.68742],\n",
       " 22: [18.7144, 38.9742, 79.12, 2.67532],\n",
       " 14: [17.7736, 41.8812, 28.52, 2.69964],\n",
       " 43: [18.8222, 37.745, 456.32, 2.65143],\n",
       " 46: [18.3028, 38.7357, 114.08, 2.47467],\n",
       " 54: [15.8234, 46.2297, 41.4, 2.65143],\n",
       " 21: [19.1456, 38.1213, 108.56, 2.69964],\n",
       " 25: [18.3616, 40.0945, 97.52, 2.67532],\n",
       " 8: [18.8124, 40.2976, 108.56, 2.68742],\n",
       " 52: [17.5384, 40.568, 60.72, 2.65143],\n",
       " 23: [19.3808, 38.1213, 224.48, 2.68742],\n",
       " 20: [20.2432, 35.9912, 128.8, 3.14243],\n",
       " 39: [19.0182, 39.6878, 150.88, 2.68742],\n",
       " 47: [18.0676, 40.1622, 655.04, 2.67532]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 2\n",
      "2 0\n",
      "2 1\n"
     ]
    }
   ],
   "source": [
    "num_sensors = 3\n",
    "adj_matrix = np.ones((num_sensors, num_sensors))\n",
    "np.fill_diagonal(adj_matrix, 0)\n",
    "temp = np.transpose(np.nonzero(adj_matrix)).reshape(1, -1)\n",
    "edge_list = np.array([np.array(temp[0][::2]) , np.array(temp[0][1::2])])\n",
    "for idx in range(edge_list.shape[1]):\n",
    "    print(edge_list[0][idx], edge_list[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', '21.5', '23'], ['2', '24.5', '20'], ['3', '19.5', '19'], ['4', '22.5', '15'], ['5', '24.5', '12'], ['6', '19.5', '12'], ['7', '22.5', '8'], ['8', '24.5', '4'], ['9', '21.5', '2'], ['10', '19.5', '5'], ['11', '16.5', '3'], ['12', '13.5', '1'], ['13', '12.5', '5'], ['14', '8.5', '6'], ['15', '5.5', '3'], ['16', '1.5', '2'], ['17', '1.5', '8'], ['18', '5.5', '10'], ['19', '3.5', '13'], ['20', '0.5', '17'], ['21', '4.5', '18'], ['22', '1.5', '23'], ['23', '6', '24'], ['24', '1.5', '30'], ['25', '4.5', '30'], ['26', '7.5', '31'], ['27', '8.5', '26'], ['28', '10.5', '31'], ['29', '12.5', '26'], ['30', '13.5', '31'], ['31', '15.5', '28'], ['32', '17.5', '31'], ['33', '19.5', '26'], ['34', '21.5', '30'], ['35', '24.5', '27'], ['36', '26.5', '31'], ['37', '27.5', '26'], ['38', '30.5', '31'], ['39', '30.5', '26'], ['40', '33.5', '28'], ['41', '36.5', '30'], ['42', '39.5', '30'], ['43', '35.5', '24'], ['44', '40.5', '22'], ['45', '37.5', '19'], ['46', '34.5', '16'], ['47', '39.5', '14'], ['48', '35.5', '10'], ['49', '39.5', '6'], ['50', '38.5', '1'], ['51', '35.5', '4'], ['52', '31.5', '6'], ['53', '28.5', '5'], ['54', '26.5', '2']]\n"
     ]
    }
   ],
   "source": [
    "positions = open('labapp3-positions.txt', 'r').read().strip()\n",
    "positions = positions.split(\"\\n\")\n",
    "positions = [x.split(\" \") for x in positions]\n",
    "print(positions)\n",
    "positions = [[float(x) for x in data] for data in positions]\n",
    "position = {}\n",
    "for dt in positions:\n",
    "    position[int(dt[0])] = (dt[1], dt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (21.5, 23.0),\n",
       " 2: (24.5, 20.0),\n",
       " 3: (19.5, 19.0),\n",
       " 4: (22.5, 15.0),\n",
       " 5: (24.5, 12.0),\n",
       " 6: (19.5, 12.0),\n",
       " 7: (22.5, 8.0),\n",
       " 8: (24.5, 4.0),\n",
       " 9: (21.5, 2.0),\n",
       " 10: (19.5, 5.0),\n",
       " 11: (16.5, 3.0),\n",
       " 12: (13.5, 1.0),\n",
       " 13: (12.5, 5.0),\n",
       " 14: (8.5, 6.0),\n",
       " 15: (5.5, 3.0),\n",
       " 16: (1.5, 2.0),\n",
       " 17: (1.5, 8.0),\n",
       " 18: (5.5, 10.0),\n",
       " 19: (3.5, 13.0),\n",
       " 20: (0.5, 17.0),\n",
       " 21: (4.5, 18.0),\n",
       " 22: (1.5, 23.0),\n",
       " 23: (6.0, 24.0),\n",
       " 24: (1.5, 30.0),\n",
       " 25: (4.5, 30.0),\n",
       " 26: (7.5, 31.0),\n",
       " 27: (8.5, 26.0),\n",
       " 28: (10.5, 31.0),\n",
       " 29: (12.5, 26.0),\n",
       " 30: (13.5, 31.0),\n",
       " 31: (15.5, 28.0),\n",
       " 32: (17.5, 31.0),\n",
       " 33: (19.5, 26.0),\n",
       " 34: (21.5, 30.0),\n",
       " 35: (24.5, 27.0),\n",
       " 36: (26.5, 31.0),\n",
       " 37: (27.5, 26.0),\n",
       " 38: (30.5, 31.0),\n",
       " 39: (30.5, 26.0),\n",
       " 40: (33.5, 28.0),\n",
       " 41: (36.5, 30.0),\n",
       " 42: (39.5, 30.0),\n",
       " 43: (35.5, 24.0),\n",
       " 44: (40.5, 22.0),\n",
       " 45: (37.5, 19.0),\n",
       " 46: (34.5, 16.0),\n",
       " 47: (39.5, 14.0),\n",
       " 48: (35.5, 10.0),\n",
       " 49: (39.5, 6.0),\n",
       " 50: (38.5, 1.0),\n",
       " 51: (35.5, 4.0),\n",
       " 52: (31.5, 6.0),\n",
       " 53: (28.5, 5.0),\n",
       " 54: (26.5, 2.0)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 24, 23,  2,  5, 25, 26, 30, 33, 42, 19, 39])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(data_dict[30].keys()))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "adj = cdist(np.array(positions)[:, 1:], np.array(positions)[:, 1:], metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 54)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.load(os.path.join(\"data/adj_mat.npy\"))\n",
    "X = np.load(os.path.join(\"data/node_values.npy\")).transpose(\n",
    "    (1, 2, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207, 2, 34272)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape \n",
    "#sensors, total outputs time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_x = np.zeros((54,len(list(data_dict.keys())), 4))\n",
    "\n",
    "time_idx = {}\n",
    "for idx, key in enumerate(data_dict.keys()):\n",
    "    time_idx[key] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2313683/2313683 [00:06<00:00, 355962.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "data = f.split(\"\\n\")\n",
    "\n",
    "for i in tqdm(data):\n",
    "    try:\n",
    "        X = [float(x) for x in i.split(\" \")]\n",
    "        our_x[int(X[1])][time_idx[int(X[0]) -1]][0] = X[2]\n",
    "        our_x[int(X[1])][time_idx[int(X[0]) -1]][1] = X[3]\n",
    "        our_x[int(X[1])][time_idx[int(X[0]) -1]][2] = X[4]\n",
    "        our_x[int(X[1])][time_idx[int(X[0]) -1]][3] = X[5]\n",
    "\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 4, 210800)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 210800, 4)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_x = our_x.transpose(0, 2, 1)\n",
    "our_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array to a file\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "# save to npy file\n",
    "save('data/our_data.npy', our_x)\n",
    "save('data/our_adj.npy', adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{25: [18.93, 38.8039, 97.52, 2.68742],\n",
       " 30: [18.4596, 41.5789, 114.08, 2.68742],\n",
       " 10: [19.273, 39.9252, 75.44, 2.67532],\n",
       " 26: [18.8614, 40.9055, 121.44, 2.68742],\n",
       " 31: [18.9496, 40.3652, 150.88, 2.69964],\n",
       " 32: [18.734, 39.9929, 121.44, 2.69964],\n",
       " 21: [19.5964, 37.162, 114.08, 2.69964]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(os.path.join(\"data/node_values.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67.        ,  0.69444444],\n",
       "       [63.5       ,  0.69444444],\n",
       "       [65.5       ,  0.69444444],\n",
       "       [53.375     ,  0.69444444],\n",
       "       [40.625     ,  0.69444444],\n",
       "       [63.        ,  0.69444444],\n",
       "       [31.625     ,  0.69444444],\n",
       "       [62.25      ,  0.69444444],\n",
       "       [65.375     ,  0.69444444],\n",
       "       [68.375     ,  0.69444444],\n",
       "       [62.625     ,  0.69444444],\n",
       "       [62.625     ,  0.69444444],\n",
       "       [15.        ,  0.69444444],\n",
       "       [68.375     ,  0.69444444],\n",
       "       [65.875     ,  0.69444444],\n",
       "       [46.        ,  0.69444444],\n",
       "       [19.375     ,  0.69444444],\n",
       "       [56.375     ,  0.69444444],\n",
       "       [57.25      ,  0.69444444],\n",
       "       [47.        ,  0.69444444],\n",
       "       [68.5       ,  0.69444444],\n",
       "       [64.625     ,  0.69444444],\n",
       "       [49.125     ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [54.125     ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [34.14285714,  0.69444444],\n",
       "       [62.        ,  0.69444444],\n",
       "       [68.375     ,  0.69444444],\n",
       "       [44.375     ,  0.69444444],\n",
       "       [60.75      ,  0.69444444],\n",
       "       [64.625     ,  0.69444444],\n",
       "       [68.125     ,  0.69444444],\n",
       "       [53.        ,  0.69444444],\n",
       "       [63.        ,  0.69444444],\n",
       "       [65.375     ,  0.69444444],\n",
       "       [69.75      ,  0.69444444],\n",
       "       [50.25      ,  0.69444444],\n",
       "       [ 8.625     ,  0.69444444],\n",
       "       [38.25      ,  0.69444444],\n",
       "       [65.5       ,  0.69444444],\n",
       "       [61.25      ,  0.69444444],\n",
       "       [29.875     ,  0.69444444],\n",
       "       [31.375     ,  0.69444444],\n",
       "       [68.75      ,  0.69444444],\n",
       "       [67.5       ,  0.69444444],\n",
       "       [60.        ,  0.69444444],\n",
       "       [36.125     ,  0.69444444],\n",
       "       [13.625     ,  0.69444444],\n",
       "       [61.5       ,  0.69444444],\n",
       "       [64.375     ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [43.375     ,  0.69444444],\n",
       "       [52.        ,  0.69444444],\n",
       "       [38.        ,  0.69444444],\n",
       "       [61.25      ,  0.69444444],\n",
       "       [39.85714286,  0.69444444],\n",
       "       [27.625     ,  0.69444444],\n",
       "       [63.75      ,  0.69444444],\n",
       "       [63.75      ,  0.69444444],\n",
       "       [38.125     ,  0.69444444],\n",
       "       [22.375     ,  0.69444444],\n",
       "       [55.5       ,  0.69444444],\n",
       "       [47.875     ,  0.69444444],\n",
       "       [66.        ,  0.69444444],\n",
       "       [58.625     ,  0.69444444],\n",
       "       [66.5       ,  0.69444444],\n",
       "       [69.625     ,  0.69444444],\n",
       "       [67.875     ,  0.69444444],\n",
       "       [42.5       ,  0.69444444],\n",
       "       [63.125     ,  0.69444444],\n",
       "       [62.75      ,  0.69444444],\n",
       "       [28.625     ,  0.69444444],\n",
       "       [25.5       ,  0.69444444],\n",
       "       [24.25      ,  0.69444444],\n",
       "       [44.625     ,  0.69444444],\n",
       "       [22.75      ,  0.69444444],\n",
       "       [67.875     ,  0.69444444],\n",
       "       [67.875     ,  0.69444444],\n",
       "       [66.875     ,  0.69444444],\n",
       "       [69.25      ,  0.69444444],\n",
       "       [40.5       ,  0.69444444],\n",
       "       [26.375     ,  0.69444444],\n",
       "       [66.625     ,  0.69444444],\n",
       "       [59.        ,  0.69444444],\n",
       "       [67.625     ,  0.69444444],\n",
       "       [35.875     ,  0.69444444],\n",
       "       [60.25      ,  0.69444444],\n",
       "       [66.5       ,  0.69444444],\n",
       "       [65.5       ,  0.69444444],\n",
       "       [68.25      ,  0.69444444],\n",
       "       [30.875     ,  0.69444444],\n",
       "       [67.625     ,  0.69444444],\n",
       "       [21.        ,  0.69444444],\n",
       "       [42.        ,  0.69444444],\n",
       "       [57.5       ,  0.69444444],\n",
       "       [69.25      ,  0.69444444],\n",
       "       [24.75      ,  0.69444444],\n",
       "       [58.375     ,  0.69444444],\n",
       "       [68.125     ,  0.69444444],\n",
       "       [59.57142857,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [61.25      ,  0.69444444],\n",
       "       [66.5       ,  0.69444444],\n",
       "       [61.5       ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [65.25      ,  0.69444444],\n",
       "       [67.25      ,  0.69444444],\n",
       "       [67.625     ,  0.69444444],\n",
       "       [54.5       ,  0.69444444],\n",
       "       [67.5       ,  0.69444444],\n",
       "       [27.375     ,  0.69444444],\n",
       "       [31.83333333,  0.69444444],\n",
       "       [40.875     ,  0.69444444],\n",
       "       [67.125     ,  0.69444444],\n",
       "       [67.25      ,  0.69444444],\n",
       "       [65.25      ,  0.69444444],\n",
       "       [65.625     ,  0.69444444],\n",
       "       [68.375     ,  0.69444444],\n",
       "       [23.75      ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [65.375     ,  0.69444444],\n",
       "       [69.25      ,  0.69444444],\n",
       "       [67.        ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [61.75      ,  0.69444444],\n",
       "       [67.875     ,  0.69444444],\n",
       "       [26.25      ,  0.69444444],\n",
       "       [61.33333333,  0.69444444],\n",
       "       [60.5       ,  0.69444444],\n",
       "       [16.625     ,  0.69444444],\n",
       "       [67.75      ,  0.69444444],\n",
       "       [38.        ,  0.69444444],\n",
       "       [26.28571429,  0.69444444],\n",
       "       [62.625     ,  0.69444444],\n",
       "       [39.875     ,  0.69444444],\n",
       "       [42.        ,  0.69444444],\n",
       "       [40.75      ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [60.        ,  0.69444444],\n",
       "       [37.875     ,  0.69444444],\n",
       "       [51.875     ,  0.69444444],\n",
       "       [58.5       ,  0.69444444],\n",
       "       [35.5       ,  0.69444444],\n",
       "       [42.25      ,  0.69444444],\n",
       "       [20.625     ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [62.125     ,  0.69444444],\n",
       "       [ 9.875     ,  0.69444444],\n",
       "       [51.25      ,  0.69444444],\n",
       "       [60.25      ,  0.69444444],\n",
       "       [64.        ,  0.69444444],\n",
       "       [64.375     ,  0.69444444],\n",
       "       [23.875     ,  0.69444444],\n",
       "       [64.375     ,  0.69444444],\n",
       "       [58.375     ,  0.69444444],\n",
       "       [22.125     ,  0.69444444],\n",
       "       [18.625     ,  0.69444444],\n",
       "       [63.875     ,  0.69444444],\n",
       "       [23.125     ,  0.69444444],\n",
       "       [37.625     ,  0.69444444],\n",
       "       [20.75      ,  0.69444444],\n",
       "       [27.75      ,  0.69444444],\n",
       "       [25.75      ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [30.        ,  0.69444444],\n",
       "       [66.625     ,  0.69444444],\n",
       "       [60.375     ,  0.69444444],\n",
       "       [63.        ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [65.25      ,  0.69444444],\n",
       "       [36.375     ,  0.69444444],\n",
       "       [62.        ,  0.69444444],\n",
       "       [ 0.        ,  0.69444444],\n",
       "       [32.375     ,  0.69444444],\n",
       "       [68.75      ,  0.69444444],\n",
       "       [60.375     ,  0.69444444],\n",
       "       [67.        ,  0.69444444],\n",
       "       [58.        ,  0.69444444],\n",
       "       [46.75      ,  0.69444444],\n",
       "       [60.75      ,  0.69444444],\n",
       "       [61.875     ,  0.69444444],\n",
       "       [67.25      ,  0.69444444],\n",
       "       [61.        ,  0.69444444],\n",
       "       [66.5       ,  0.69444444],\n",
       "       [43.75      ,  0.69444444],\n",
       "       [21.25      ,  0.69444444],\n",
       "       [40.25      ,  0.69444444],\n",
       "       [65.        ,  0.69444444],\n",
       "       [45.5       ,  0.69444444],\n",
       "       [25.75      ,  0.69444444],\n",
       "       [61.25      ,  0.69444444],\n",
       "       [22.5       ,  0.69444444],\n",
       "       [61.5       ,  0.69444444],\n",
       "       [64.25      ,  0.69444444],\n",
       "       [20.375     ,  0.69444444],\n",
       "       [25.75      ,  0.69444444],\n",
       "       [17.875     ,  0.69444444],\n",
       "       [63.66666667,  0.69444444],\n",
       "       [67.875     ,  0.69444444],\n",
       "       [68.875     ,  0.69444444],\n",
       "       [64.25      ,  0.69444444],\n",
       "       [60.        ,  0.69444444],\n",
       "       [55.625     ,  0.69444444],\n",
       "       [61.25      ,  0.69444444],\n",
       "       [62.75      ,  0.69444444]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of data * number of sensors * per sensor data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2313683/2313683 [00:21<00:00, 109060.38it/s]\n",
      "100%|██████████| 2219803/2219803 [00:07<00:00, 291090.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "from torch_geometric.loader import DataLoader\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "\n",
    "\n",
    "positions = open('labapp3-positions.txt', 'r').read().strip()\n",
    "positions = positions.split(\"\\n\")\n",
    "positions = [x.split(\" \") for x in positions]\n",
    "#print(positions)\n",
    "positions = [[float(x) for x in data] for data in positions]\n",
    "position = {}\n",
    "for dt in positions:\n",
    "    position[int(dt[0])] = (dt[1], dt[2])\n",
    "\n",
    "from tqdm import tqdm\n",
    "data_file = \"labapp3-data-new.txt\"\n",
    "f = open(data_file, \"r\").read()\n",
    "data = f.split(\"\\n\")\n",
    "X = []\n",
    "for i in tqdm(data):\n",
    "    try:\n",
    "        X.append([float(x) for x in i.split(\" \")])\n",
    "        if np.isnan(X[-1]).any():\n",
    "            X.pop()\n",
    "    except:\n",
    "        pass\n",
    "MIN_S = np.min(np.array(X), axis=0)\n",
    "MAX_S = np.max(np.array(X), axis=0)\n",
    "num_sensors = 54\n",
    "from collections import defaultdict\n",
    "data_dict = defaultdict(dict)\n",
    "for dt in tqdm(X):\n",
    "    if np.isnan(dt[1]):\n",
    "        continue\n",
    "    if int(dt[1]) < num_sensors:\n",
    "        data_dict[int(dt[0])][int(dt[1])] = dt[2:]\n",
    "\n",
    "adj_matrix = np.ones((num_sensors, num_sensors))\n",
    "np.fill_diagonal(adj_matrix, 0)\n",
    "temp = np.transpose(np.nonzero(adj_matrix)).reshape(1, -1)\n",
    "edge_list = np.array([np.array(temp[0][::2]) , np.array(temp[0][1::2])])\n",
    "edge_attr = []\n",
    "for idx in range(edge_list.shape[1]):\n",
    "    fm, to = edge_list[0][idx]+1, edge_list[1][idx]+1\n",
    "    edge_attr.append(math.sqrt((position[fm][0]-position[to][0])**2 + (position[fm][1]-position[to][1])**2))\n",
    "edge_attr = np.array(edge_attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m g\u001b[39m.\u001b[39mtrain_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mFalse\u001b[39;00m]\u001b[39m*\u001b[39mnum_sensors)\n\u001b[1;32m     24\u001b[0m g\u001b[39m.\u001b[39mtrain_mask[np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(data_dict[time]\u001b[39m.\u001b[39mkeys()))\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m g\u001b[39m.\u001b[39;49mtrain_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(g\u001b[39m.\u001b[39mtrain_mask)\n\u001b[1;32m     26\u001b[0m \u001b[39m#print(\"train mask shape\", g.train_mask.shape)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m#print(\"input shape\",g.x.shape)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m g\u001b[39m.\u001b[39mtest_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(([\u001b[39mTrue\u001b[39;00m]\u001b[39m*\u001b[39mnum_sensors))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workshop/lib/python3.9/site-packages/torch_geometric/data/data.py:435\u001b[0m, in \u001b[0;36mData.__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    433\u001b[0m     propobj\u001b[39m.\u001b[39mfset(\u001b[39mself\u001b[39m, value)\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[39msetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store, key, value)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workshop/lib/python3.9/site-packages/torch_geometric/data/storage.py:76\u001b[0m, in \u001b[0;36mBaseStorage.__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m     75\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m[key] \u001b[39m=\u001b[39m value\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workshop/lib/python3.9/site-packages/torch_geometric/data/storage.py:91\u001b[0m, in \u001b[0;36mBaseStorage.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mapping[key]\n\u001b[1;32m     90\u001b[0m \u001b[39melif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mapping[key] \u001b[39m=\u001b[39m value\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graphs = []\n",
    "for time in data_dict.keys():\n",
    "\n",
    "    sensor_data = np.zeros((num_sensors, 4))\n",
    "    for sensor_id in data_dict[time]:\n",
    "        try:\n",
    "            sensor_data[sensor_id-1] = (np.array(data_dict[time][sensor_id])-MIN_S[2:])/(MAX_S[2:]-MIN_S[2:])\n",
    "        except:\n",
    "            #print(np.array(data_dict[time][sensor_id]))\n",
    "            #print(MIN_S[2:])\n",
    "            continue\n",
    "\n",
    "    g = Data(x=torch.tensor(sensor_data, dtype=torch.float), \n",
    "             edge_index=torch.tensor(edge_list,dtype=torch.long), \n",
    "             y=torch.tensor(sensor_data, dtype=torch.float),\n",
    "             edge_attr=torch.tensor(edge_attr.reshape(-1, 1), dtype=torch.float))\n",
    "             \n",
    "    \n",
    "    #g = Data(x=torch.rand((num_cols, num_cols), dtype=torch.float), edge_index=torch.tensor(edge_list,dtype=torch.long))\n",
    "    \n",
    "    #g.y = torch.tensor(np.array(sensor).reshape(-1, 1), dtype=torch.float)\n",
    "    \n",
    "    g.train_mask = np.array([False]*num_sensors)\n",
    "    g.train_mask[np.array(list(data_dict[time].keys()))-1] = True\n",
    "    g.train_mask = torch.tensor(g.train_mask)\n",
    "    #print(\"train mask shape\", g.train_mask.shape)\n",
    "    #print(\"input shape\",g.x.shape)\n",
    "\n",
    "    g.test_mask = np.array(([True]*num_sensors))\n",
    "    g.test_mask[np.array(list(data_dict[time].keys()))-1] = False\n",
    "    g.test_mask = torch.tensor(g.test_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({22: [18.6066, 40.8043, 82.8, 2.61639], 44: [17.2444, 44.9121, 32.2, 2.63964], 23: [19.3024, 39.7217, 235.52, 2.65143], 10: [18.734, 42.685, 7.36, 2.63964], 31: [18.4106, 43.5522, 143.52, 2.65143], 26: [18.3714, 43.9844, 121.44, 2.65143], 29: [18.8418, 41.6461, 180.32, 2.65143], 25: [18.4988, 41.6461, 97.52, 2.65143], 19: [18.9104, 40.5004, 136.16, 2.65143], 41: [18.0872, 43.3525, 121.44, 2.63964], 24: [18.7438, 41.0404, 143.52, 2.67532], 7: [19.126, 41.0404, 9.2, 2.65143]},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data_dict[258480])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "sensor_data = np.zeros((num_sensors, 4))\n",
    "print(sensor_data[53])\n",
    "sensor_data[53] = np.array([1, 2, 3, 4])\n",
    "print(sensor_data[53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3593/3593 [00:31<00:00, 114.82it/s]\n",
      " 20%|██        | 1/5 [00:31<02:05, 31.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 245.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3593/3593 [00:40<00:00, 89.08it/s]\n",
      " 40%|████      | 2/5 [01:11<01:49, 36.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train Loss: 241.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3593/3593 [00:39<00:00, 90.52it/s]\n",
      " 60%|██████    | 3/5 [01:51<01:16, 38.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Loss: 240.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3593/3593 [00:43<00:00, 82.81it/s]\n",
      " 80%|████████  | 4/5 [02:34<00:40, 40.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Train Loss: 239.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3593/3593 [00:49<00:00, 72.62it/s]\n",
      "100%|██████████| 5/5 [03:24<00:00, 40.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train Loss: 239.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "from torch_geometric.loader import DataLoader\n",
    "torch.manual_seed(12345)\n",
    "df = pd.read_csv(\"df_env.csv\")\n",
    "\n",
    "df = df.dropna(how='all')\n",
    "df = df.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "df_list = df.values.tolist()\n",
    "num_cols = 23\n",
    "graphs = []\n",
    "for sensor in df_list:\n",
    "\n",
    "    updated_sensor = []\n",
    "    for idx, val in enumerate(sensor):\n",
    "        temp = [0]*num_cols\n",
    "        if math.isnan(val):\n",
    "            updated_sensor.append(temp[:])\n",
    "            continue\n",
    "        else:\n",
    "            #print(\"comes\", idx, val)\n",
    "            temp[idx] = val\n",
    "        #print(temp.shape)\n",
    "        #print(temp)\n",
    "        updated_sensor.append(temp[:])\n",
    "    #if len(updated_sensor)==8:\n",
    "    #    print(sensor)\n",
    "    #    print(updated_sensor)\n",
    "    #print(\"input shape\",np.array(updated_sensor).shape)\n",
    "    adj_matrix = np.ones((num_cols, num_cols))\n",
    "    a = np.argwhere(np.isnan(sensor)).reshape(1, -1)[0]\n",
    "\n",
    "    for idx in a:\n",
    "        for i in range(num_cols):\n",
    "            adj_matrix[idx][i] = 0\n",
    "            adj_matrix[i][idx] = 0\n",
    "    \n",
    "    np.fill_diagonal(adj_matrix, 0)\n",
    "    #if len(a):\n",
    "    #    print(a)\n",
    "    #    print(adj_matrix)\n",
    "    temp = np.transpose(np.nonzero(adj_matrix)).reshape(1, -1)\n",
    "    edge_list = np.array([np.array(temp[0][::2]) , np.array(temp[0][1::2])])\n",
    "    g = Data(x=torch.tensor(np.array(updated_sensor), dtype=torch.float), edge_index=torch.tensor(edge_list,dtype=torch.long))\n",
    "    #g = Data(x=torch.rand((num_cols, num_cols), dtype=torch.float), edge_index=torch.tensor(edge_list,dtype=torch.long))\n",
    "    \n",
    "    g.y = torch.tensor(np.array(sensor).reshape(-1, 1), dtype=torch.float)\n",
    "    #g.y = torch.rand((num_cols, 1), dtype=torch.float)\n",
    "    g.train_mask = np.array([True]*num_cols)\n",
    "    g.train_mask[np.argwhere(np.isnan(sensor))] = False\n",
    "    g.train_mask = torch.tensor(g.train_mask)\n",
    "    #print(\"train mask shape\", g.train_mask.shape)\n",
    "    #print(\"input shape\",g.x.shape)\n",
    "    g.test_mask = np.array(([False]*num_cols))\n",
    "    g.test_mask[np.argwhere(np.isnan(sensor))] = True\n",
    "    g.test_mask = torch.tensor(g.test_mask)\n",
    "\n",
    "    #replace g.x tensor with nan values to 0\n",
    "    #print(g.x)\n",
    "    #g.x[torch.isnan(g.x)] = 0\n",
    "    #g.y[torch.isnan(g.x)] = 0\n",
    "    #print(g.x.shape)\n",
    "    graphs.append(g)\n",
    "\n",
    "#graphs = graphs[:20]\n",
    "\n",
    "#loader = DataLoader(graphs, batch_size=10, shuffle=False)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_cols, 16)\n",
    "        self.conv2 = GCNConv(16, 32)\n",
    "        self.conv3 = GCNConv(32, 32)\n",
    "        self.conv4 = GCNConv(32, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"Graph Attention Network\"\"\"\n",
    "    def __init__(self, dim_in=num_cols,dim_h=64, dim_out=1, heads=8):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n",
    "        self.bn1 = nn.BatchNorm1d(dim_h*heads)\n",
    "        self.gats = nn.ModuleList(\n",
    "            [\n",
    "                GATv2Conv(dim_h*heads, dim_h*heads)\n",
    "                for _ in range(2)\n",
    "             ]\n",
    "        )\n",
    "        self.bn2s = nn.ModuleList(\n",
    "            [\n",
    "                nn.BatchNorm1d(dim_h*heads)\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #print(x.shape)\n",
    "        h = self.gat1(x, edge_index)\n",
    "        #print(h.shape)\n",
    "        #h = self.bn1(h)\n",
    "        h = F.relu(h)\n",
    "        #print(h.shape)\n",
    "        #for gat, bn in zip(self.gats, self.bn2s):\n",
    "        #    h = gat(h, edge_index)\n",
    "        #    h = bn(h)\n",
    "        #    h = F.elu(h)\n",
    "        #h = F.elu(h)\n",
    "\n",
    "        h , att_weights = self.gat2(h, edge_index, return_attention_weights=True)\n",
    "        #h = torch.sigmoid(h)\n",
    "        #print(h.shape)\n",
    "        return h, att_weights\n",
    "\n",
    "\n",
    "class GAT_V2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATv2Conv(num_cols, 120, heads=8)\n",
    "        self.gat2 = GATv2Conv(120*8, 1, heads=1)\n",
    "        #self.conv1 = GCNConv(data.num_node_features, 120)\n",
    "        #self.linear = nn.Linear(120, 5)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.gat2(x, edge_index) \n",
    "        #logits, attention_weights = x\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graphs, optimizer, criterion, n_epochs=100):\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        total_loss = 0\n",
    "\n",
    "        for graph in tqdm(graphs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            #print(\"before comes\", graph.x.shape, graph.y.shape, graph.train_mask.shape, graph.edge_index)\n",
    "            out = model(graph)\n",
    "            #print(\"comes\", graph.x.shape,out.shape, graph.y.shape, graph.train_mask.shape)\n",
    "            loss = criterion(out[graph.train_mask] , graph.y[graph.train_mask])\n",
    "            #print(loss)\n",
    "            if math.isnan(loss.item()):\n",
    "                print(epoch,graph.train_mask, out[graph.train_mask], graph.y[graph.train_mask], criterion(out[graph.train_mask], graph.y[graph.train_mask]))\n",
    "                print(\"######### SHOULD NOT HAPPEN ######### \", loss)\n",
    "                break\n",
    "                #print(graph.x, graph.train_mask)\n",
    "            #print(loss)\n",
    "            total_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {total_loss:.3f}')\n",
    "\n",
    "    return model\n",
    "model = GAT_V2().to('cpu')\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.L1Loss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "model = train_node_classifier(model, graphs, optimizer, criterion, n_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8058],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.8031],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.2350],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.2350],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.2621],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.2621],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.9325],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.9325],\n",
      "        [   nan]])\n",
      "tensor([ True, False, False,  True, False, False,  True, False, False,  True,\n",
      "        False, False,  True, False, False,  True, False, False,  True, False,\n",
      "        False,  True, False])\n",
      "tensor([False,  True,  True, False,  True,  True, False,  True,  True, False,\n",
      "         True,  True, False,  True,  True, False,  True,  True, False,  True,\n",
      "         True, False,  True])\n"
     ]
    }
   ],
   "source": [
    "print(graphs[4].y)\n",
    "print(graphs[4].train_mask)\n",
    "print(graphs[4].test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3593it [00:13, 266.44it/s]\n"
     ]
    }
   ],
   "source": [
    "out_final = []\n",
    "def eval_node_classifier(model, graphs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, graph in tqdm(enumerate(graphs)):\n",
    "            out = model(graph)\n",
    "            # copy the output to a new list\n",
    "            out_list = graph.y.clone()\n",
    "            \n",
    "            out_list[graph.test_mask] = out[graph.test_mask]\n",
    "            out_list = out_list.reshape(1, -1)[0].detach().numpy().tolist()\n",
    "            out_final.append(out_list)\n",
    "\n",
    "eval_node_classifier(model, graphs)\n",
    "\n",
    "# save out_final to csv with column names\n",
    "mod_df = pd.DataFrame(out_final, columns=df.columns)\n",
    "mod_df.to_csv('modified_env.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8058],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.8031],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.2350],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.2350],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.2621],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.2621],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.9325],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.9325],\n",
       "        [   nan]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs[4].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean.Temperature_60', 'grad.Temperature_60', 'sd.Temperature_60',\n",
       "       'mean.Temperature_480', 'grad.Temperature_480', 'sd.Temperature_480',\n",
       "       'mean.Humidity_60', 'grad.Humidity_60', 'sd.Humidity_60',\n",
       "       'mean.Humidity_480', 'grad.Humidity_480', 'sd.Humidity_480',\n",
       "       'mean.Winvel_60', 'grad.Winvel_60', 'sd.Winvel_60', 'mean.Winvel_480',\n",
       "       'grad.Winvel_480', 'sd.Winvel_480', 'mean.Solar_60', 'grad.Solar_60',\n",
       "       'sd.Solar_60', 'mean.Solar_480', 'grad.Solar_480'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.805787980556488, 0.3065623939037323, 0.2926788330078125, 0.8030948042869568, 0.3040156364440918, 0.2632540762424469, 0.23497267067432404, 0.29013514518737793, 0.26361241936683655, 0.23497267067432404, 0.30663588643074036, 0.2609925866127014, 0.26213592290878296, 0.26588839292526245, 0.3064268231391907, 0.26213592290878296, 0.24456262588500977, 0.25844889879226685, 0.9325337409973145, 0.27355900406837463, 0.24704766273498535, 0.9325337409973145, 0.2800537645816803]\n",
      "tensor([[0.8058],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.8031],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.2350],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.2350],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.2621],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.2621],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.9325],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [0.9325],\n",
      "        [   nan]])\n"
     ]
    }
   ],
   "source": [
    "print(out_final[4])\n",
    "print(graphs[4].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor([[0.8058],\n",
      "        [0.2858],\n",
      "        [0.2903],\n",
      "        [0.8031],\n",
      "        [0.3068],\n",
      "        [0.2899],\n",
      "        [0.2350],\n",
      "        [0.3041],\n",
      "        [0.2852],\n",
      "        [0.2350],\n",
      "        [0.2635],\n",
      "        [0.3040],\n",
      "        [0.2621],\n",
      "        [0.2634],\n",
      "        [0.2878],\n",
      "        [0.2621],\n",
      "        [0.3069],\n",
      "        [0.3042],\n",
      "        [0.9325],\n",
      "        [0.2903],\n",
      "        [0.3045],\n",
      "        [0.9325],\n",
      "        [0.2712]], grad_fn=<IndexPutBackward0>) tensor([False,  True,  True, False,  True,  True, False,  True,  True, False,\n",
      "         True,  True, False,  True,  True, False,  True,  True, False,  True,\n",
      "         True, False,  True])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for idx,  graph in enumerate(graphs):\n",
    "        \n",
    "        #check if test mask contains true\n",
    "        if graph.test_mask.any():\n",
    "            print(idx, graph.y, graph.test_mask)\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8273],\n",
       "        [0.6343],\n",
       "        [0.2725],\n",
       "        [0.8229],\n",
       "        [0.6272],\n",
       "        [0.1574],\n",
       "        [0.2268],\n",
       "        [0.6690],\n",
       "        [0.0384],\n",
       "        [0.2274],\n",
       "        [0.7108],\n",
       "        [0.0379],\n",
       "        [0.1917],\n",
       "        [0.5438],\n",
       "        [0.3401],\n",
       "        [0.1916],\n",
       "        [0.5475],\n",
       "        [0.4264],\n",
       "        [0.9454],\n",
       "        [0.5079],\n",
       "        [0.0254],\n",
       "        [0.9448],\n",
       "        [0.5074]], grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs[4].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2313683/2313683 [00:15<00:00, 148503.98it/s]\n",
      "100%|██████████| 2219803/2219803 [00:06<00:00, 320780.32it/s]\n",
      "100%|██████████| 5000/5000 [00:10<00:00, 497.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "from torch_geometric.loader import DataLoader\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "\n",
    "### distance matrix calculation for using in the graph\n",
    "\n",
    "positions = open('labapp3-positions.txt', 'r').read().strip()\n",
    "positions = positions.split(\"\\n\")\n",
    "positions = [x.split(\" \") for x in positions]\n",
    "positions = [[float(x) for x in data] for data in positions]\n",
    "position = {}\n",
    "for dt in positions:\n",
    "    position[int(dt[0])] = (dt[1], dt[2])\n",
    "\n",
    "# reading the main data file for processing\n",
    "data_file = \"labapp3-data-new.txt\"\n",
    "f = open(data_file, \"r\").read()\n",
    "data = f.split(\"\\n\")\n",
    "X = []\n",
    "for i in tqdm(data):\n",
    "    try:\n",
    "        X.append([float(x) for x in i.split(\" \")])\n",
    "        if np.isnan(X[-1]).any():\n",
    "            X.pop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# MAX MIN array calculation for each column for normalizing the data later\n",
    "\n",
    "MIN_S = np.min(np.array(X), axis=0)\n",
    "MAX_S = np.max(np.array(X), axis=0)\n",
    "\n",
    "\n",
    "#creating a dictionary of data for each time stamp\n",
    "'''\n",
    "Dictionary structure:\n",
    "{time_stamp: {sensor_id_1: [sensor_data],\n",
    "             sensor_id_2: [sensor_data], ...},\n",
    "time_stamp_2: {sensor_id_1: [sensor_data], \n",
    "            sensor_id_2: [sensor_data], ...}, ...\n",
    "'''\n",
    "num_sensors = 54\n",
    "from collections import defaultdict\n",
    "data_dict = defaultdict(dict)\n",
    "for dt in tqdm(X):\n",
    "    if np.isnan(dt[1]):\n",
    "        continue\n",
    "    if int(dt[1]) < num_sensors:\n",
    "        data_dict[int(dt[0])][int(dt[1])] = dt[2:]\n",
    "\n",
    "\n",
    "# loop for creating the graph data for each time stamp\n",
    "graphs = []\n",
    "all_times = []\n",
    "# change the line below to change the number of time stamps to be used for training\n",
    "for time in tqdm(sorted(list(data_dict.keys()))[:5000]):\n",
    "    all_times.append(time)\n",
    "    #print(time)\n",
    "    # some times are not multiple of 30, so skipping them\n",
    "    #if time%30 != 0:\n",
    "    #    continue\n",
    "    sensor_data = np.zeros((num_sensors, 4))\n",
    "    for sensor_id in data_dict[time]:\n",
    "        try:\n",
    "            # converting one indexing to zero indexing\n",
    "            sensor_data[sensor_id-1] = (np.array(data_dict[time][sensor_id])-MIN_S[2:])/(MAX_S[2:]-MIN_S[2:])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # creating the adjacency matrix\n",
    "    adj_matrix = np.ones((num_sensors, num_sensors))\n",
    "\n",
    "    #list of indices of sensors which are not present in the current time stamp\n",
    "    a = [i for i in range(num_sensors) if i not in (np.array(list(data_dict[time].keys()))-1).tolist() ]\n",
    "\n",
    "    # removing the edges from the adjacency matrix for those absent sensors\n",
    "    for idx in a:\n",
    "        for i in range(num_sensors):\n",
    "            adj_matrix[idx][i] = 0\n",
    "            adj_matrix[i][idx] = 0\n",
    "\n",
    "    # removing self connections from the adjacency matrix because we want to predict the sensor data from other sensors\n",
    "    np.fill_diagonal(adj_matrix, 0)\n",
    "    # creating the edge list from the adjacency matrix with pyg graph edge list format\n",
    "    temp = np.transpose(np.nonzero(adj_matrix)).reshape(1, -1)\n",
    "    edge_list = np.array([np.array(temp[0][::2]) , np.array(temp[0][1::2])])\n",
    "\n",
    "    # calculating the edge attributes for the graph\n",
    "    edge_attr = []\n",
    "    for idx in range(edge_list.shape[1]):\n",
    "        fm, to = edge_list[0][idx]+1, edge_list[1][idx]+1\n",
    "        edge_attr.append(math.sqrt((position[fm][0]-position[to][0])**2 + (position[fm][1]-position[to][1])**2))\n",
    "    edge_attr = np.array(edge_attr)\n",
    "\n",
    "    g = Data(x=torch.tensor(sensor_data, dtype=torch.float), \n",
    "             edge_index=torch.tensor(edge_list,dtype=torch.long), \n",
    "             y=torch.tensor(sensor_data, dtype=torch.float),\n",
    "             edge_attr=torch.tensor(edge_attr.reshape(-1, 1), dtype=torch.float))\n",
    "    \n",
    "    # creating the train mask so that the loss is only calculated for the present sensors\n",
    "    g.train_mask = np.array([False]*num_sensors)\n",
    "    g.train_mask[np.array(list(data_dict[time].keys()))-1] = True\n",
    "    g.train_mask = torch.tensor(g.train_mask)\n",
    " \n",
    "\n",
    "    # creating the test mask so that the loss is not calculated for the absent sensors\n",
    "    g.test_mask = np.array(([True]*num_sensors))\n",
    "    g.test_mask[np.array(list(data_dict[time].keys()))-1] = False\n",
    "    g.test_mask = torch.tensor(g.test_mask)\n",
    "\n",
    "    graphs.append(g)\n",
    "\n",
    "print(len(graphs))\n",
    "#graphs = graphs[:10000]\n",
    "\n",
    "# creaitng the dataloader for the graphs so that they can be used for batching and efficient training\n",
    "loader = DataLoader(graphs, batch_size=10, shuffle=False)\n",
    "\n",
    "# three models are used for comparison\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_sensors, 16)\n",
    "        self.conv2 = GCNConv(16, 32)\n",
    "        self.conv3 = GCNConv(32, 32)\n",
    "        self.conv4 = GCNConv(32, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"Graph Attention Network\"\"\"\n",
    "    def __init__(self, dim_in=num_sensors,dim_h=64, dim_out=4, heads=8):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n",
    "        self.bn1 = nn.BatchNorm1d(dim_h*heads)\n",
    "        self.gats = nn.ModuleList(\n",
    "            [\n",
    "                GATv2Conv(dim_h*heads, dim_h*heads)\n",
    "                for _ in range(2)\n",
    "             ]\n",
    "        )\n",
    "        self.bn2s = nn.ModuleList(\n",
    "            [\n",
    "                nn.BatchNorm1d(dim_h*heads)\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #print(x.shape)\n",
    "        h = self.gat1(x, edge_index)\n",
    "        #print(h.shape)\n",
    "        #h = self.bn1(h)\n",
    "        h = F.relu(h)\n",
    "        #print(h.shape)\n",
    "        #for gat, bn in zip(self.gats, self.bn2s):\n",
    "        #    h = gat(h, edge_index)\n",
    "        #    h = bn(h)\n",
    "        #    h = F.elu(h)\n",
    "        #h = F.elu(h)\n",
    "\n",
    "        h , att_weights = self.gat2(h, edge_index, return_attention_weights=True)\n",
    "        #h = torch.sigmoid(h)\n",
    "        #print(h.shape)\n",
    "        return h, att_weights\n",
    "\n",
    "\n",
    "class GAT_V2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATv2Conv(4, 120, heads=8)\n",
    "        self.gat2 = GATv2Conv(120*8, 4, heads=1)\n",
    "        #self.conv1 = GCNConv(data.num_node_features, 120)\n",
    "        #self.linear = nn.Linear(120, 5)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.gat2(x, edge_index) \n",
    "        #logits, attention_weights = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "        False,  True, False,  True, False,  True, False,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True, False, False, False,  True,\n",
      "         True, False,  True, False,  True, False,  True, False, False, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False,  True, False])\n",
      "[1, 3, 4, 6, 7, 8, 9, 10, 12, 14, 16, 18, 19, 20, 21, 22, 23, 24, 26, 30, 31, 33, 35, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53]\n"
     ]
    }
   ],
   "source": [
    "idx = 2017\n",
    "print(graphs[idx].train_mask)\n",
    "print(sorted(list(data_dict[(idx+1)*30].keys())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60510"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2016+1)*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 3, 4, 6, 7, 8, 9, 10, 12, 14, 16, 18, 19, 20, 21, 22, 23, 24, 26, 30, 31, 33, 35, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53]\n",
    "tensor([False, False,  True,  True, False, False,  True, False,  True,  True,\n",
    "         True, False, False,  True, False,  True, False,  True,  True,  True,\n",
    "         True,  True, False,  True,  True,  True, False, False,  True,  True,\n",
    "         True, False, False, False,  True,  True,  True, False, False,  True,\n",
    "        False,  True,  True, False,  True,  True,  True,  True, False,  True,\n",
    "         True,  True, False, False])\n",
    "3, 4, 7, 9, 10, 11, 14, 16, 18, 19, 20, 21, 22, 24, 25, 26, 29, 30, 31, 35, 36, 37, 40, 42, 43, 45, 46, 47, 48, 50, 51, 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2323it [00:28, 82.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 85\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39m#model = train_node_classifier(model, loader, optimizer, criterion, n_epochs=10)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 85\u001b[0m eval_node_classifier(model, graphs)\n\u001b[1;32m     87\u001b[0m \u001b[39m# save out_final to csv with column names\u001b[39;00m\n\u001b[1;32m     88\u001b[0m mod_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(out_final, columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnodeid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m ,\u001b[39m\"\u001b[39m\u001b[39mhumidity\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlight\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvoltage\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[41], line 48\u001b[0m, in \u001b[0;36meval_node_classifier\u001b[0;34m(model, graphs)\u001b[0m\n\u001b[1;32m     46\u001b[0m g\u001b[39m.\u001b[39medge_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(edge_list,dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m     47\u001b[0m g\u001b[39m.\u001b[39medge_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(edge_attr\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m---> 48\u001b[0m out \u001b[39m=\u001b[39m model(g)\n\u001b[1;32m     49\u001b[0m out_list \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     50\u001b[0m time \u001b[39m=\u001b[39m all_times[idx]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[40], line 205\u001b[0m, in \u001b[0;36mGAT_V2.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m    204\u001b[0m     x, edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index\n\u001b[0;32m--> 205\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgat1(x, edge_index)\n\u001b[1;32m    206\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    207\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workshop/lib/python3.9/site-packages/torch_geometric/nn/conv/gatv2_conv.py:236\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    231\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThe usage of \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_attr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39madd_self_loops\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39msimultaneously is currently not yet supported for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m in a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseTensor\u001b[39m\u001b[39m'\u001b[39m\u001b[39m form\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    235\u001b[0m \u001b[39m# propagate_type: (x: PairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49m(x_l, x_r), edge_attr\u001b[39m=\u001b[39;49medge_attr,\n\u001b[1;32m    237\u001b[0m                      size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    239\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alpha\n\u001b[1;32m    240\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alpha \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workshop/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:437\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         msg_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 437\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmsg_kwargs)\n\u001b[1;32m    438\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    439\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (msg_kwargs, ), out)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workshop/lib/python3.9/site-packages/torch_geometric/nn/conv/gatv2_conv.py:273\u001b[0m, in \u001b[0;36mGATv2Conv.message\u001b[0;34m(self, x_j, x_i, edge_attr, index, ptr, size_i)\u001b[0m\n\u001b[1;32m    270\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m edge_attr\n\u001b[1;32m    272\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mleaky_relu(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnegative_slope)\n\u001b[0;32m--> 273\u001b[0m alpha \u001b[39m=\u001b[39m (x \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt)\u001b[39m.\u001b[39;49msum(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    274\u001b[0m alpha \u001b[39m=\u001b[39m softmax(alpha, index, ptr, size_i)\n\u001b[1;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alpha \u001b[39m=\u001b[39m alpha\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_node_classifier(model, graphs, optimizer, criterion, n_epochs=100):\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        total_loss = 0\n",
    "\n",
    "        for graph in tqdm(graphs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            #print(\"before comes\", graph.x.shape, graph.y.shape, graph.train_mask.shape, graph.edge_index)\n",
    "            out = model(graph)\n",
    "            #print(\"comes\", graph.x.shape,out.shape, graph.y.shape, graph.train_mask.shape)\n",
    "            loss = criterion(out[graph.train_mask] , graph.y[graph.train_mask])\n",
    "            #print(loss)\n",
    "            if math.isnan(loss.item()):\n",
    "                print(epoch,graph.train_mask, out[graph.train_mask], graph.y[graph.train_mask], criterion(out[graph.train_mask], graph.y[graph.train_mask]))\n",
    "                print(\"######### SHOULD NOT HAPPEN ######### \", loss)\n",
    "                break\n",
    "                #print(graph.x, graph.train_mask)\n",
    "            #print(loss)\n",
    "            total_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {total_loss:.3f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# redundant code (can be removed but kept for reference)\n",
    "adj_matrix = np.ones((num_sensors, num_sensors))\n",
    "np.fill_diagonal(adj_matrix, 0)\n",
    "temp = np.transpose(np.nonzero(adj_matrix)).reshape(1, -1)\n",
    "edge_list = np.array([np.array(temp[0][::2]) , np.array(temp[0][1::2])])\n",
    "\n",
    "edge_attr = []\n",
    "for idx in range(edge_list.shape[1]):\n",
    "    fm, to = edge_list[0][idx]+1, edge_list[1][idx]+1\n",
    "    edge_attr.append(math.sqrt((position[fm][0]-position[to][0])**2 + (position[fm][1]-position[to][1])**2))\n",
    "edge_attr = np.array(edge_attr)\n",
    "\n",
    "out_final = []\n",
    "def eval_node_classifier(model, graphs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, graph in tqdm(enumerate(graphs)):\n",
    "            g = copy.deepcopy(graph)    \n",
    "            g.edge_index = torch.tensor(edge_list,dtype=torch.long)\n",
    "            g.edge_attr = torch.tensor(edge_attr.reshape(-1, 1), dtype=torch.float)\n",
    "            out = model(g)\n",
    "            out_list = out.detach().numpy().tolist()\n",
    "            time = all_times[idx]\n",
    "            for i in range(num_sensors):\n",
    "                # we want a prediction for each sensor\n",
    "                if not graph.train_mask[i]:\n",
    "                    # if this sensor is not in the training set, we want to predict and use the imputed value\n",
    "                    out_final.append([time, i+1]+out_list[i])\n",
    "                else:\n",
    "                    try:\n",
    "                        # if this sensor is in the training set, we want to use the ground truth value\n",
    "                        out_final.append([time, i+1]+((np.array(data_dict[time][i+1])-MIN_S[2:])/(MAX_S[2:]-MIN_S[2:])).tolist())\n",
    "                    except Exception as e:\n",
    "                        # print error e with trace\n",
    "                        print(str(e))\n",
    "                        print((idx+1)*30)\n",
    "                        print(sorted(list(data_dict[(idx+1)*30].keys())))\n",
    "                        print(g.train_mask)\n",
    "                        print(g.test_mask)\n",
    "\n",
    "\n",
    "\n",
    "#model = GCN().to('cpu')\n",
    "#model = GAT().to('cpu')\n",
    "model = GAT_V2().to('cpu')\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.L1Loss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "#model = train_node_classifier(model, loader, optimizer, criterion, n_epochs=10)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "eval_node_classifier(model, graphs)\n",
    "\n",
    "# save out_final to csv with column names\n",
    "mod_df = pd.DataFrame(out_final, columns=[\"time\", \"nodeid\", \"temperature\" ,\"humidity\", \"light\", \"voltage\"])\n",
    "mod_df.to_csv('modified_env.csv', index=False)\n",
    "\n",
    "'''\n",
    "30,1,0.20009765028953552,0.0059858085587620735,0.7404278516769409,-0.0762353166937828\n",
    "30,2,0.2406468242406845,0.01273853424936533,0.838738203048706,-0.046470265835523605\n",
    "30,3,0.1382274133896898,0.988967344623328,0.027390438247011956,0.8541260865461349\n",
    "30,4,0.2236645221710205,-0.018838858231902122,0.7683352828025818,-0.02886117435991764\n",
    "30,5,0.214375302195549,-0.050112172961235046,0.812414824962616,-0.05435218662023544\n",
    "30,6,0.1387359423352706,0.9889371603446336,0.06573705179282868,0.8388215635376892\n",
    "30,7,0.2642294466495514,-0.007743816822767258,0.8069591522216797,-0.06117755547165871\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,  True,\n",
       "        False, False, False,  True,  True,  True,  True, False, False, False,\n",
       "         True, False, False,  True, False, False, False, False, False,  True,\n",
       "        False, False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs[0].train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3843, 24)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "df = pd.read_csv(\"df_env.csv\")\n",
    "all_ids = df.ID.to_list()\n",
    "df = df.drop(columns=['ID'])\n",
    "print(df.shape)\n",
    "df_list = df.values.tolist()\n",
    "\n",
    "#df = df.dropna(how='all')\n",
    "df = df.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "num_cols = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c8154bc89338aa61183803fddff66c28cb5c44ca94b859a7dd1cc91d2b6a781"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
